{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19018a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "from numpy.typing import NDArray\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Optimizer, AdamW\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import Module\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35261dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = None\n",
    "y = None\n",
    "with open('../dirs.yaml', 'r') as file:\n",
    "    dirs = yaml.safe_load(file)['homework-1']\n",
    "    X = pd.read_csv(dirs['train']['x'])\n",
    "    y = pd.read_csv(dirs['train']['y'])\n",
    "    \n",
    "df = pd.merge(X, y, on='Unnamed: 0').drop(columns=['Unnamed: 0'])\n",
    "X = df.drop(columns=['year'])\n",
    "y = df['year']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2d4213a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.47518</td>\n",
       "      <td>-14.34414</td>\n",
       "      <td>40.54872</td>\n",
       "      <td>-9.10171</td>\n",
       "      <td>16.77175</td>\n",
       "      <td>-17.77113</td>\n",
       "      <td>-16.50156</td>\n",
       "      <td>-4.09543</td>\n",
       "      <td>2.49723</td>\n",
       "      <td>-0.46428</td>\n",
       "      <td>...</td>\n",
       "      <td>-115.62803</td>\n",
       "      <td>-13.79660</td>\n",
       "      <td>31.60436</td>\n",
       "      <td>28.95927</td>\n",
       "      <td>-25.93164</td>\n",
       "      <td>67.64670</td>\n",
       "      <td>-25.76691</td>\n",
       "      <td>-81.90373</td>\n",
       "      <td>-61.48682</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.17393</td>\n",
       "      <td>-7.02208</td>\n",
       "      <td>-30.36086</td>\n",
       "      <td>-2.41924</td>\n",
       "      <td>2.15406</td>\n",
       "      <td>-8.44502</td>\n",
       "      <td>-1.68191</td>\n",
       "      <td>-8.71434</td>\n",
       "      <td>-7.83802</td>\n",
       "      <td>-5.58019</td>\n",
       "      <td>...</td>\n",
       "      <td>24.58950</td>\n",
       "      <td>-36.95682</td>\n",
       "      <td>0.73922</td>\n",
       "      <td>-0.06330</td>\n",
       "      <td>53.75838</td>\n",
       "      <td>-81.05330</td>\n",
       "      <td>8.42811</td>\n",
       "      <td>12.08694</td>\n",
       "      <td>-1.91676</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.58141</td>\n",
       "      <td>26.03203</td>\n",
       "      <td>-4.92774</td>\n",
       "      <td>35.71620</td>\n",
       "      <td>8.53080</td>\n",
       "      <td>3.73167</td>\n",
       "      <td>-7.98443</td>\n",
       "      <td>-7.43976</td>\n",
       "      <td>-1.69797</td>\n",
       "      <td>10.75028</td>\n",
       "      <td>...</td>\n",
       "      <td>-366.07968</td>\n",
       "      <td>-62.38201</td>\n",
       "      <td>113.48188</td>\n",
       "      <td>4.72741</td>\n",
       "      <td>181.64459</td>\n",
       "      <td>-134.46216</td>\n",
       "      <td>8.50795</td>\n",
       "      <td>94.15573</td>\n",
       "      <td>-8.47276</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.15615</td>\n",
       "      <td>-17.77029</td>\n",
       "      <td>-32.30961</td>\n",
       "      <td>-21.03778</td>\n",
       "      <td>12.80330</td>\n",
       "      <td>-13.48031</td>\n",
       "      <td>-3.14951</td>\n",
       "      <td>-7.62647</td>\n",
       "      <td>-4.48901</td>\n",
       "      <td>-4.29075</td>\n",
       "      <td>...</td>\n",
       "      <td>-79.64532</td>\n",
       "      <td>-77.08169</td>\n",
       "      <td>38.88094</td>\n",
       "      <td>28.52025</td>\n",
       "      <td>24.17783</td>\n",
       "      <td>-86.62542</td>\n",
       "      <td>-1.19418</td>\n",
       "      <td>-74.73449</td>\n",
       "      <td>-17.28130</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.54855</td>\n",
       "      <td>78.77563</td>\n",
       "      <td>-23.29877</td>\n",
       "      <td>98.60192</td>\n",
       "      <td>-30.11496</td>\n",
       "      <td>26.94220</td>\n",
       "      <td>-8.87771</td>\n",
       "      <td>-3.23280</td>\n",
       "      <td>-1.04841</td>\n",
       "      <td>31.69655</td>\n",
       "      <td>...</td>\n",
       "      <td>-252.61021</td>\n",
       "      <td>118.93768</td>\n",
       "      <td>-155.87390</td>\n",
       "      <td>51.85666</td>\n",
       "      <td>-365.15815</td>\n",
       "      <td>59.34936</td>\n",
       "      <td>52.47311</td>\n",
       "      <td>99.00695</td>\n",
       "      <td>-10.18840</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>52.33593</td>\n",
       "      <td>2.89543</td>\n",
       "      <td>56.90252</td>\n",
       "      <td>8.96987</td>\n",
       "      <td>-25.76965</td>\n",
       "      <td>-15.71307</td>\n",
       "      <td>3.50595</td>\n",
       "      <td>-8.96479</td>\n",
       "      <td>1.19235</td>\n",
       "      <td>-4.97747</td>\n",
       "      <td>...</td>\n",
       "      <td>37.47755</td>\n",
       "      <td>-14.58737</td>\n",
       "      <td>120.49606</td>\n",
       "      <td>-14.30169</td>\n",
       "      <td>-75.96685</td>\n",
       "      <td>40.57184</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>60.78252</td>\n",
       "      <td>-15.17380</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50.93086</td>\n",
       "      <td>2.40169</td>\n",
       "      <td>35.79261</td>\n",
       "      <td>-0.07677</td>\n",
       "      <td>-40.96232</td>\n",
       "      <td>-17.04343</td>\n",
       "      <td>-10.91591</td>\n",
       "      <td>-11.32212</td>\n",
       "      <td>4.69807</td>\n",
       "      <td>-2.25197</td>\n",
       "      <td>...</td>\n",
       "      <td>-110.61446</td>\n",
       "      <td>-60.67350</td>\n",
       "      <td>4.09050</td>\n",
       "      <td>1.17609</td>\n",
       "      <td>-89.90531</td>\n",
       "      <td>50.88912</td>\n",
       "      <td>-6.94646</td>\n",
       "      <td>174.95178</td>\n",
       "      <td>26.32797</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44.11978</td>\n",
       "      <td>11.85193</td>\n",
       "      <td>57.67591</td>\n",
       "      <td>-14.54233</td>\n",
       "      <td>4.04202</td>\n",
       "      <td>6.18560</td>\n",
       "      <td>-12.82692</td>\n",
       "      <td>4.09408</td>\n",
       "      <td>11.87608</td>\n",
       "      <td>2.09057</td>\n",
       "      <td>...</td>\n",
       "      <td>-64.08981</td>\n",
       "      <td>396.40911</td>\n",
       "      <td>312.23906</td>\n",
       "      <td>10.09221</td>\n",
       "      <td>-220.19327</td>\n",
       "      <td>144.47076</td>\n",
       "      <td>29.26308</td>\n",
       "      <td>-717.40632</td>\n",
       "      <td>45.79519</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>40.79316</td>\n",
       "      <td>39.50647</td>\n",
       "      <td>27.37837</td>\n",
       "      <td>-11.96658</td>\n",
       "      <td>-5.43111</td>\n",
       "      <td>-6.25519</td>\n",
       "      <td>0.21025</td>\n",
       "      <td>5.37451</td>\n",
       "      <td>9.27321</td>\n",
       "      <td>10.06320</td>\n",
       "      <td>...</td>\n",
       "      <td>30.89832</td>\n",
       "      <td>185.94793</td>\n",
       "      <td>55.47131</td>\n",
       "      <td>12.10047</td>\n",
       "      <td>-35.04364</td>\n",
       "      <td>15.84014</td>\n",
       "      <td>4.84509</td>\n",
       "      <td>48.40314</td>\n",
       "      <td>-7.48619</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>38.31416</td>\n",
       "      <td>-28.59876</td>\n",
       "      <td>-1.87686</td>\n",
       "      <td>-10.30072</td>\n",
       "      <td>11.44784</td>\n",
       "      <td>1.15831</td>\n",
       "      <td>-20.47703</td>\n",
       "      <td>-2.45890</td>\n",
       "      <td>9.27336</td>\n",
       "      <td>-1.97484</td>\n",
       "      <td>...</td>\n",
       "      <td>-44.63124</td>\n",
       "      <td>93.55093</td>\n",
       "      <td>84.37248</td>\n",
       "      <td>-10.88730</td>\n",
       "      <td>-33.68677</td>\n",
       "      <td>-57.70108</td>\n",
       "      <td>13.22433</td>\n",
       "      <td>-51.66323</td>\n",
       "      <td>11.02160</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  37.47518 -14.34414  40.54872  -9.10171  16.77175 -17.77113 -16.50156   \n",
       "1  48.17393  -7.02208 -30.36086  -2.41924   2.15406  -8.44502  -1.68191   \n",
       "2  36.58141  26.03203  -4.92774  35.71620   8.53080   3.73167  -7.98443   \n",
       "3  41.15615 -17.77029 -32.30961 -21.03778  12.80330 -13.48031  -3.14951   \n",
       "4  40.54855  78.77563 -23.29877  98.60192 -30.11496  26.94220  -8.87771   \n",
       "5  52.33593   2.89543  56.90252   8.96987 -25.76965 -15.71307   3.50595   \n",
       "6  50.93086   2.40169  35.79261  -0.07677 -40.96232 -17.04343 -10.91591   \n",
       "7  44.11978  11.85193  57.67591 -14.54233   4.04202   6.18560 -12.82692   \n",
       "8  40.79316  39.50647  27.37837 -11.96658  -5.43111  -6.25519   0.21025   \n",
       "9  38.31416 -28.59876  -1.87686 -10.30072  11.44784   1.15831 -20.47703   \n",
       "\n",
       "          7         8         9  ...         81         82         83  \\\n",
       "0  -4.09543   2.49723  -0.46428  ... -115.62803  -13.79660   31.60436   \n",
       "1  -8.71434  -7.83802  -5.58019  ...   24.58950  -36.95682    0.73922   \n",
       "2  -7.43976  -1.69797  10.75028  ... -366.07968  -62.38201  113.48188   \n",
       "3  -7.62647  -4.48901  -4.29075  ...  -79.64532  -77.08169   38.88094   \n",
       "4  -3.23280  -1.04841  31.69655  ... -252.61021  118.93768 -155.87390   \n",
       "5  -8.96479   1.19235  -4.97747  ...   37.47755  -14.58737  120.49606   \n",
       "6 -11.32212   4.69807  -2.25197  ... -110.61446  -60.67350    4.09050   \n",
       "7   4.09408  11.87608   2.09057  ...  -64.08981  396.40911  312.23906   \n",
       "8   5.37451   9.27321  10.06320  ...   30.89832  185.94793   55.47131   \n",
       "9  -2.45890   9.27336  -1.97484  ...  -44.63124   93.55093   84.37248   \n",
       "\n",
       "         84         85         86        87         88        89  year  \n",
       "0  28.95927  -25.93164   67.64670 -25.76691  -81.90373 -61.48682  2001  \n",
       "1  -0.06330   53.75838  -81.05330   8.42811   12.08694  -1.91676  2006  \n",
       "2   4.72741  181.64459 -134.46216   8.50795   94.15573  -8.47276  1989  \n",
       "3  28.52025   24.17783  -86.62542  -1.19418  -74.73449 -17.28130  1997  \n",
       "4  51.85666 -365.15815   59.34936  52.47311   99.00695 -10.18840  2001  \n",
       "5 -14.30169  -75.96685   40.57184   0.99512   60.78252 -15.17380  2009  \n",
       "6   1.17609  -89.90531   50.88912  -6.94646  174.95178  26.32797  2009  \n",
       "7  10.09221 -220.19327  144.47076  29.26308 -717.40632  45.79519  2000  \n",
       "8  12.10047  -35.04364   15.84014   4.84509   48.40314  -7.48619  1976  \n",
       "9 -10.88730  -33.68677  -57.70108  13.22433  -51.66323  11.02160  1994  \n",
       "\n",
       "[10 rows x 91 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e47266f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>43.394558</td>\n",
       "      <td>1.540279</td>\n",
       "      <td>8.378243</td>\n",
       "      <td>1.315349</td>\n",
       "      <td>-6.476035</td>\n",
       "      <td>-9.503415</td>\n",
       "      <td>-2.243164</td>\n",
       "      <td>-1.660698</td>\n",
       "      <td>3.543946</td>\n",
       "      <td>1.892996</td>\n",
       "      <td>...</td>\n",
       "      <td>-72.008182</td>\n",
       "      <td>41.154440</td>\n",
       "      <td>37.892338</td>\n",
       "      <td>0.145765</td>\n",
       "      <td>17.576243</td>\n",
       "      <td>-28.502296</td>\n",
       "      <td>4.474666</td>\n",
       "      <td>17.717701</td>\n",
       "      <td>1.227647</td>\n",
       "      <td>1998.366714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.074562</td>\n",
       "      <td>51.578894</td>\n",
       "      <td>35.742553</td>\n",
       "      <td>16.622971</td>\n",
       "      <td>22.823521</td>\n",
       "      <td>12.926176</td>\n",
       "      <td>14.700348</td>\n",
       "      <td>8.035388</td>\n",
       "      <td>10.629817</td>\n",
       "      <td>6.602191</td>\n",
       "      <td>...</td>\n",
       "      <td>171.932584</td>\n",
       "      <td>120.075095</td>\n",
       "      <td>96.748418</td>\n",
       "      <td>16.162963</td>\n",
       "      <td>115.706029</td>\n",
       "      <td>180.463987</td>\n",
       "      <td>13.592096</td>\n",
       "      <td>185.335542</td>\n",
       "      <td>22.018997</td>\n",
       "      <td>11.048088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.199890</td>\n",
       "      <td>-302.031900</td>\n",
       "      <td>-257.525600</td>\n",
       "      <td>-120.723150</td>\n",
       "      <td>-142.160680</td>\n",
       "      <td>-60.198620</td>\n",
       "      <td>-100.602550</td>\n",
       "      <td>-51.443820</td>\n",
       "      <td>-75.539550</td>\n",
       "      <td>-31.344160</td>\n",
       "      <td>...</td>\n",
       "      <td>-1976.846950</td>\n",
       "      <td>-791.832320</td>\n",
       "      <td>-1237.931680</td>\n",
       "      <td>-227.608010</td>\n",
       "      <td>-2678.193680</td>\n",
       "      <td>-3059.906060</td>\n",
       "      <td>-100.618700</td>\n",
       "      <td>-5000.654060</td>\n",
       "      <td>-286.031200</td>\n",
       "      <td>1922.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39.970260</td>\n",
       "      <td>-25.603507</td>\n",
       "      <td>-11.773080</td>\n",
       "      <td>-8.490435</td>\n",
       "      <td>-20.716735</td>\n",
       "      <td>-18.524397</td>\n",
       "      <td>-10.651808</td>\n",
       "      <td>-6.402985</td>\n",
       "      <td>-2.573275</td>\n",
       "      <td>-2.446435</td>\n",
       "      <td>...</td>\n",
       "      <td>-138.560657</td>\n",
       "      <td>-21.424297</td>\n",
       "      <td>-4.443660</td>\n",
       "      <td>-7.027105</td>\n",
       "      <td>-32.236598</td>\n",
       "      <td>-101.305695</td>\n",
       "      <td>-2.563620</td>\n",
       "      <td>-60.364940</td>\n",
       "      <td>-8.873683</td>\n",
       "      <td>1994.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>44.282770</td>\n",
       "      <td>8.800720</td>\n",
       "      <td>10.226865</td>\n",
       "      <td>-0.582025</td>\n",
       "      <td>-5.808040</td>\n",
       "      <td>-11.254920</td>\n",
       "      <td>-2.007255</td>\n",
       "      <td>-1.516640</td>\n",
       "      <td>3.658655</td>\n",
       "      <td>1.763500</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.984525</td>\n",
       "      <td>29.218730</td>\n",
       "      <td>33.576620</td>\n",
       "      <td>0.770135</td>\n",
       "      <td>15.522650</td>\n",
       "      <td>-22.050410</td>\n",
       "      <td>3.021435</td>\n",
       "      <td>6.982460</td>\n",
       "      <td>-0.050725</td>\n",
       "      <td>2002.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.843418</td>\n",
       "      <td>36.610743</td>\n",
       "      <td>29.596557</td>\n",
       "      <td>9.035892</td>\n",
       "      <td>7.858092</td>\n",
       "      <td>-2.198952</td>\n",
       "      <td>6.725500</td>\n",
       "      <td>3.111740</td>\n",
       "      <td>9.935210</td>\n",
       "      <td>6.245860</td>\n",
       "      <td>...</td>\n",
       "      <td>14.405743</td>\n",
       "      <td>89.374030</td>\n",
       "      <td>78.710667</td>\n",
       "      <td>8.380725</td>\n",
       "      <td>66.429958</td>\n",
       "      <td>50.378898</td>\n",
       "      <td>9.912000</td>\n",
       "      <td>86.542642</td>\n",
       "      <td>9.664982</td>\n",
       "      <td>2006.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>57.408630</td>\n",
       "      <td>240.617010</td>\n",
       "      <td>318.868960</td>\n",
       "      <td>143.841600</td>\n",
       "      <td>142.305480</td>\n",
       "      <td>68.300090</td>\n",
       "      <td>147.965260</td>\n",
       "      <td>52.331120</td>\n",
       "      <td>78.149440</td>\n",
       "      <td>36.596180</td>\n",
       "      <td>...</td>\n",
       "      <td>1081.951340</td>\n",
       "      <td>1473.745210</td>\n",
       "      <td>1458.580210</td>\n",
       "      <td>199.121500</td>\n",
       "      <td>1620.749740</td>\n",
       "      <td>1879.332480</td>\n",
       "      <td>352.359540</td>\n",
       "      <td>3165.372860</td>\n",
       "      <td>245.209770</td>\n",
       "      <td>2011.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1             2             3             4  \\\n",
       "count  14000.000000  14000.000000  14000.000000  14000.000000  14000.000000   \n",
       "mean      43.394558      1.540279      8.378243      1.315349     -6.476035   \n",
       "std        6.074562     51.578894     35.742553     16.622971     22.823521   \n",
       "min        7.199890   -302.031900   -257.525600   -120.723150   -142.160680   \n",
       "25%       39.970260    -25.603507    -11.773080     -8.490435    -20.716735   \n",
       "50%       44.282770      8.800720     10.226865     -0.582025     -5.808040   \n",
       "75%       47.843418     36.610743     29.596557      9.035892      7.858092   \n",
       "max       57.408630    240.617010    318.868960    143.841600    142.305480   \n",
       "\n",
       "                  5             6             7             8             9  \\\n",
       "count  14000.000000  14000.000000  14000.000000  14000.000000  14000.000000   \n",
       "mean      -9.503415     -2.243164     -1.660698      3.543946      1.892996   \n",
       "std       12.926176     14.700348      8.035388     10.629817      6.602191   \n",
       "min      -60.198620   -100.602550    -51.443820    -75.539550    -31.344160   \n",
       "25%      -18.524397    -10.651808     -6.402985     -2.573275     -2.446435   \n",
       "50%      -11.254920     -2.007255     -1.516640      3.658655      1.763500   \n",
       "75%       -2.198952      6.725500      3.111740      9.935210      6.245860   \n",
       "max       68.300090    147.965260     52.331120     78.149440     36.596180   \n",
       "\n",
       "       ...            81            82            83            84  \\\n",
       "count  ...  14000.000000  14000.000000  14000.000000  14000.000000   \n",
       "mean   ...    -72.008182     41.154440     37.892338      0.145765   \n",
       "std    ...    171.932584    120.075095     96.748418     16.162963   \n",
       "min    ...  -1976.846950   -791.832320  -1237.931680   -227.608010   \n",
       "25%    ...   -138.560657    -21.424297     -4.443660     -7.027105   \n",
       "50%    ...    -52.984525     29.218730     33.576620      0.770135   \n",
       "75%    ...     14.405743     89.374030     78.710667      8.380725   \n",
       "max    ...   1081.951340   1473.745210   1458.580210    199.121500   \n",
       "\n",
       "                 85            86            87            88            89  \\\n",
       "count  14000.000000  14000.000000  14000.000000  14000.000000  14000.000000   \n",
       "mean      17.576243    -28.502296      4.474666     17.717701      1.227647   \n",
       "std      115.706029    180.463987     13.592096    185.335542     22.018997   \n",
       "min    -2678.193680  -3059.906060   -100.618700  -5000.654060   -286.031200   \n",
       "25%      -32.236598   -101.305695     -2.563620    -60.364940     -8.873683   \n",
       "50%       15.522650    -22.050410      3.021435      6.982460     -0.050725   \n",
       "75%       66.429958     50.378898      9.912000     86.542642      9.664982   \n",
       "max     1620.749740   1879.332480    352.359540   3165.372860    245.209770   \n",
       "\n",
       "               year  \n",
       "count  14000.000000  \n",
       "mean    1998.366714  \n",
       "std       11.048088  \n",
       "min     1922.000000  \n",
       "25%     1994.000000  \n",
       "50%     2002.000000  \n",
       "75%     2006.000000  \n",
       "max     2011.000000  \n",
       "\n",
       "[8 rows x 91 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f3552",
   "metadata": {},
   "source": [
    "**Выводы:**\n",
    " 1) Наибольшее число песен в диапазоне от 1994 до 2006\n",
    " 3) У некоторых данных достаточно большая дисперсия\n",
    "\n",
    "Из [предоставленого источника](https://archive.ics.uci.edu/dataset/203/yearpredictionmsd) можно узнать, что в датасете:\n",
    "90 атрибутов, 12 = средний тон, 78 = ковариация тона\n",
    "Первое значение — год (целевая переменная), диапазон от 1922 до 2011.\n",
    "Характеристики, извлеченные из 'timbre' характеристик API The Echo Nest.\n",
    "Берется среднее значение и ковариацию для всех 'сегментов', каждый сегмент\n",
    "описывается 12-мерным вектором timbre. \n",
    "\n",
    "**Пояснение:**\n",
    "\n",
    "Пусть трек состоит из $N$ сегментов, тогда можно записать в виде матрицы:\n",
    "$$\n",
    "\\begin{pmatrix} t_{1,1} & ... & t_{1,12} \\\\ ... & ... & .. \\\\ t_{N,1} & ... & t_{N,12} \\end{pmatrix}\n",
    "$$\n",
    "Тогда:\n",
    "\n",
    "Первые 12 значений:\n",
    "$$\n",
    "\\mu = \\frac{1}{N}\\sum\\limits_{i=1}^NT_i \\\\\n",
    "\\Sigma = \\frac{1}{N-1}(T - 1\\mu^T)^T(T - 1\\mu^T)\n",
    "$$\n",
    "\n",
    "78 - чтобы не повторять значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6f9f435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14000 entries, 0 to 13999\n",
      "Data columns (total 91 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       14000 non-null  float64\n",
      " 1   1       14000 non-null  float64\n",
      " 2   2       14000 non-null  float64\n",
      " 3   3       14000 non-null  float64\n",
      " 4   4       14000 non-null  float64\n",
      " 5   5       14000 non-null  float64\n",
      " 6   6       14000 non-null  float64\n",
      " 7   7       14000 non-null  float64\n",
      " 8   8       14000 non-null  float64\n",
      " 9   9       14000 non-null  float64\n",
      " 10  10      14000 non-null  float64\n",
      " 11  11      14000 non-null  float64\n",
      " 12  12      14000 non-null  float64\n",
      " 13  13      14000 non-null  float64\n",
      " 14  14      14000 non-null  float64\n",
      " 15  15      14000 non-null  float64\n",
      " 16  16      14000 non-null  float64\n",
      " 17  17      14000 non-null  float64\n",
      " 18  18      14000 non-null  float64\n",
      " 19  19      14000 non-null  float64\n",
      " 20  20      14000 non-null  float64\n",
      " 21  21      14000 non-null  float64\n",
      " 22  22      14000 non-null  float64\n",
      " 23  23      14000 non-null  float64\n",
      " 24  24      14000 non-null  float64\n",
      " 25  25      14000 non-null  float64\n",
      " 26  26      14000 non-null  float64\n",
      " 27  27      14000 non-null  float64\n",
      " 28  28      14000 non-null  float64\n",
      " 29  29      14000 non-null  float64\n",
      " 30  30      14000 non-null  float64\n",
      " 31  31      14000 non-null  float64\n",
      " 32  32      14000 non-null  float64\n",
      " 33  33      14000 non-null  float64\n",
      " 34  34      14000 non-null  float64\n",
      " 35  35      14000 non-null  float64\n",
      " 36  36      14000 non-null  float64\n",
      " 37  37      14000 non-null  float64\n",
      " 38  38      14000 non-null  float64\n",
      " 39  39      14000 non-null  float64\n",
      " 40  40      14000 non-null  float64\n",
      " 41  41      14000 non-null  float64\n",
      " 42  42      14000 non-null  float64\n",
      " 43  43      14000 non-null  float64\n",
      " 44  44      14000 non-null  float64\n",
      " 45  45      14000 non-null  float64\n",
      " 46  46      14000 non-null  float64\n",
      " 47  47      14000 non-null  float64\n",
      " 48  48      14000 non-null  float64\n",
      " 49  49      14000 non-null  float64\n",
      " 50  50      14000 non-null  float64\n",
      " 51  51      14000 non-null  float64\n",
      " 52  52      14000 non-null  float64\n",
      " 53  53      14000 non-null  float64\n",
      " 54  54      14000 non-null  float64\n",
      " 55  55      14000 non-null  float64\n",
      " 56  56      14000 non-null  float64\n",
      " 57  57      14000 non-null  float64\n",
      " 58  58      14000 non-null  float64\n",
      " 59  59      14000 non-null  float64\n",
      " 60  60      14000 non-null  float64\n",
      " 61  61      14000 non-null  float64\n",
      " 62  62      14000 non-null  float64\n",
      " 63  63      14000 non-null  float64\n",
      " 64  64      14000 non-null  float64\n",
      " 65  65      14000 non-null  float64\n",
      " 66  66      14000 non-null  float64\n",
      " 67  67      14000 non-null  float64\n",
      " 68  68      14000 non-null  float64\n",
      " 69  69      14000 non-null  float64\n",
      " 70  70      14000 non-null  float64\n",
      " 71  71      14000 non-null  float64\n",
      " 72  72      14000 non-null  float64\n",
      " 73  73      14000 non-null  float64\n",
      " 74  74      14000 non-null  float64\n",
      " 75  75      14000 non-null  float64\n",
      " 76  76      14000 non-null  float64\n",
      " 77  77      14000 non-null  float64\n",
      " 78  78      14000 non-null  float64\n",
      " 79  79      14000 non-null  float64\n",
      " 80  80      14000 non-null  float64\n",
      " 81  81      14000 non-null  float64\n",
      " 82  82      14000 non-null  float64\n",
      " 83  83      14000 non-null  float64\n",
      " 84  84      14000 non-null  float64\n",
      " 85  85      14000 non-null  float64\n",
      " 86  86      14000 non-null  float64\n",
      " 87  87      14000 non-null  float64\n",
      " 88  88      14000 non-null  float64\n",
      " 89  89      14000 non-null  float64\n",
      " 90  year    14000 non-null  int64  \n",
      "dtypes: float64(90), int64(1)\n",
      "memory usage: 9.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0dd465",
   "metadata": {},
   "source": [
    "# Базовые классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "305ad42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X: pd.DataFrame, y: pd.Series):\n",
    "        super().__init__()\n",
    "        self.X = torch.tensor(X.to_numpy(), dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "class DataLoaderGenerator:\n",
    "    def __new__(cls,\n",
    "                X: pd.DataFrame,\n",
    "                y: pd.Series,\n",
    "                batch_size: int = 64,\n",
    "                shuffle: bool = True) -> DataLoader:\n",
    "        return DataLoader(MyDataset(X, y), batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0143157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self,\n",
    "                 num_epochs: int,\n",
    "                 device: torch.device = torch.device('cpu')):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.model = None\n",
    "        self.loss_history = []\n",
    "        self.device = device\n",
    "\n",
    "    def train(self,\n",
    "              dataloader: DataLoader,\n",
    "              model: nn.Module,\n",
    "              optimizer: torch.optim.Optimizer,\n",
    "              criterion: nn.Module) -> float:\n",
    "        self.model = model.to(self.device)\n",
    "        self.model.train()\n",
    "        with tqdm(total=len(dataloader) * self.num_epochs, unit=\"batch\", position=0, leave=True) as pbar:\n",
    "            for epoch in range(self.num_epochs):\n",
    "                running_loss = 0.0\n",
    "                for xb, yb in dataloader:\n",
    "                    xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(xb)\n",
    "                    loss = criterion(outputs, yb)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += loss.item()\n",
    "                    pbar.set_postfix({\n",
    "                        \"Epoch\": f\"{epoch + 1}/{self.num_epochs}\",\n",
    "                        \"Loss\": f\"{loss.item():.4f}\"\n",
    "                    })\n",
    "                    pbar.update()\n",
    "\n",
    "                avg_epoch_loss = running_loss / len(dataloader)\n",
    "                self.loss_history.append(avg_epoch_loss)\n",
    "                pbar.set_postfix({\"Epoch Loss\": f\"{avg_epoch_loss:.4f}\"})\n",
    "\n",
    "            pbar.close()\n",
    "\n",
    "        return self.test(dataloader)\n",
    "    \n",
    "    def test(self, dataloader: DataLoader) -> float:\n",
    "        self.model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in dataloader:\n",
    "                xb = xb.to(self.device)\n",
    "                outputs = self.model(xb).cpu().numpy()\n",
    "                y_true.extend(yb.numpy())\n",
    "                y_pred.extend(outputs)\n",
    "\n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        return mse\n",
    "\n",
    "    def plot_loss_history(self):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(self.loss_history, label=\"Train Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"MSE Loss\")\n",
    "        plt.title(\"Training Loss over Epochs\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "514ce710",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 ffn_dim: int,\n",
    "                 activation_func: Module,\n",
    "                 dropout: int= 0.1,\n",
    "                 bias: bool =False):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, ffn_dim, bias=bias)\n",
    "        self.fc2 = nn.Linear(ffn_dim, hidden_size, bias=bias)\n",
    "        self.actvation_func = activation_func\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.actvation_func(self.fc1(x)))\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a855d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 eps: float = 10**(-5),\n",
    "                 elementwise_affine: bool = True,\n",
    "                 dropout: float = 0.1,\n",
    "                 bias: bool = False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(hidden_size,\n",
    "                                 eps,\n",
    "                                 elementwise_affine,\n",
    "                                 bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer_out):\n",
    "        return self.norm(x + self.dropout(sublayer_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f37d1c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 ffn_dim: int,\n",
    "                 activation_func: Module,\n",
    "                 eps: float = 10**(-5),\n",
    "                 elementwise_affine: bool = True,\n",
    "                 dropout: float = 0.1,\n",
    "                 bias: bool = False):\n",
    "        super().__init__()\n",
    "        self.ffn = FFN(hidden_size,\n",
    "                       ffn_dim,\n",
    "                       activation_func,\n",
    "                       dropout,\n",
    "                       bias)\n",
    "        self.add_norm = AddNorm(hidden_size,\n",
    "                                eps,\n",
    "                                elementwise_affine,\n",
    "                                dropout,\n",
    "                                bias)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.add_norm(x, self.ffn(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b087d50",
   "metadata": {},
   "source": [
    "# Решение \"в лоб\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46f32502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelVer1(Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 num_layers: int,\n",
    "                 hidden_size: int,\n",
    "                 ffn_dim: int,\n",
    "                 activation_func: Module,\n",
    "                 eps: float = 10**(-5),\n",
    "                 elementwise_affine: bool = True,\n",
    "                 dropout: float = 0.1,\n",
    "                 bias: bool = False):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [MyLayer(hidden_size,\n",
    "                     ffn_dim, activation_func,\n",
    "                     eps,\n",
    "                     elementwise_affine,\n",
    "                     dropout,\n",
    "                     bias)\n",
    "             for _ in range(num_layers)]\n",
    "            \n",
    "        )\n",
    "        self.in_linear = nn.Linear(in_features, hidden_size)\n",
    "        self.out_linear = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.in_linear(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        logits = self.out_linear(x)\n",
    "        return logits.squeeze(-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c6298c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/3940 [00:02<25:32,  2.57batch/s, Epoch=1/20, Loss=3950604.0000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m criterion = nn.MSELoss()\n\u001b[32m     16\u001b[39m trainer = ModelTrainer(num_epochs=NUM_EPOCHS)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m train_mse = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m test_mse = trainer.test(test_loader)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrain MSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_mse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mtest MSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_mse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mModelTrainer.train\u001b[39m\u001b[34m(self, dataloader, model, optimizer, criterion)\u001b[39m\n\u001b[32m     24\u001b[39m outputs = model(xb)\n\u001b[32m     25\u001b[39m loss = criterion(outputs, yb)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m optimizer.step()\n\u001b[32m     28\u001b[39m running_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 20\n",
    "train_loader = DataLoaderGenerator(X_train, y_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoaderGenerator(X_test, y_test, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = ModelVer1(\n",
    "    in_features=90,\n",
    "    num_layers=20,\n",
    "    hidden_size=256,\n",
    "    ffn_dim=512,\n",
    "    activation_func=nn.ReLU()\n",
    ")\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "trainer = ModelTrainer(num_epochs=NUM_EPOCHS)\n",
    "train_mse = trainer.train(train_loader, model, optimizer, criterion)\n",
    "test_mse = trainer.test(test_loader)\n",
    "print(f\"train MSE: {train_mse}\\ntest MSE: {test_mse}\")\n",
    "trainer.plot_loss_history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
